{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(latent_dim, activation=\"relu\")\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(784, activation=\"sigmoid\"),\n",
    "            layers.Reshape((28, 28))\n",
    "        ])\n",
    "    \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "autoencoder = Autoencoder(latent_dim)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, \n",
    "                x_train, \n",
    "                epochs=10, \n",
    "                shuffle=True, \n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.title(\"original\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i])\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "x_train = x_train[..., tf.newaxis] # add extra dim to the data\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_factor = 0.2\n",
    "\n",
    "x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape)\n",
    "\n",
    "x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\n",
    "x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.) # ensures that all pixel values are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i + 1)\n",
    "    plt.title(\"original + noise\")\n",
    "    plt.imshow(tf.squeeze(x_test_noisy[i]))\n",
    "    plt.gray()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoise(Model):\n",
    "    def __init__(self):\n",
    "        super(Denoise, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(28, 28, 1)),\n",
    "            layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(8, kernel_size=2, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')\n",
    "            ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "autoencoder = Denoise()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(x_test_noisy).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "\n",
    "    # display original + noise\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.title(\"original + noise\")\n",
    "    plt.imshow(tf.squeeze(x_test_noisy[i]))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    bx = plt.subplot(2, n, i + n + 1)\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.imshow(tf.squeeze(decoded_imgs[i]))\n",
    "    plt.gray()\n",
    "    bx.get_xaxis().set_visible(False)\n",
    "    bx.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "An autoencoder is trained to minimize reconstruction error. Train an autoencoder on the normal rhythms only, then use it to reconstruct all the data. Our hypothesis is that the abnormal rhythms will have higher reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
    "raw_data = dataframe.values\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = raw_data[:, -1]\n",
    "data = raw_data[:, 0:-1]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=21\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the Data**\n",
    "\n",
    "Autoencoders are often used for anomaly detection, where the model learns to reconstruct normal patterns. Anomalies are deviations from these normal patterns. By normalizing the data to [0, 1], you provide a consistent range for the input values, making it easier for the autoencoder to identify anomalies that fall outside this range.\n",
    "\n",
    "- `(train_data - min_val)` subtracts the minimum value from each data point, ensuring that the minimum value becomes 0.\n",
    "- `(max_val - min_val)` calculates the range of values (the difference between the maximum and minimum values).\n",
    "- Dividing `(train_data - min_val)` by `(max_val - min_val)` scales the data between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data to [0, 1]\n",
    "min_val = tf.reduce_min(train_data)\n",
    "max_val = tf.reduce_max(train_data)\n",
    "\n",
    "train_data = (train_data - min_val) / (max_val - min_val) \n",
    "test_data = (test_data - min_val) / (max_val - min_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.astype(bool)\n",
    "test_labels = test_labels.astype(bool)\n",
    "\n",
    "normal_train_data = train_data[train_labels] # train_labels acts as a mask to filter only data that is \"True\" in train_labels\n",
    "normal_test_data = test_data[test_labels]\n",
    "\n",
    "anomalous_train_data = train_data[~train_labels]\n",
    "anomalous_test_data = test_data[~test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(np.arange(140), normal_train_data[0])\n",
    "plt.title(\"A Normal ECG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(np.arange(140), anomalous_train_data[0])\n",
    "plt.title(\"An Anomalous ECG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector(Model):\n",
    "    def __init__(self):\n",
    "        super(AnomalyDetector, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Dense(32, activation=\"relu\"),\n",
    "            layers.Dense(16, activation=\"relu\"),\n",
    "            layers.Dense(8, activation=\"relu\")\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(16, activation=\"relu\"),\n",
    "            layers.Dense(32, activation=\"relu\"),\n",
    "            layers.Dense(140, activation=\"sigmoid\")\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "autoencoder = AnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(normal_train_data, normal_train_data, \n",
    "          epochs=20, \n",
    "          batch_size=512,\n",
    "          validation_data=(test_data, test_data),\n",
    "          shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first occurrence of `test_data` is used as the input to the autoencoder model, while the second occurrence is used as the target output for comparison during the validation phase. \n",
    "\n",
    "The tuple structure is used to pass both the input data and the corresponding target output data.\n",
    "\n",
    "The model is not trained on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = autoencoder.encoder(normal_test_data).numpy()\n",
    "decoded_data = autoencoder.decoder(encoded_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(normal_test_data[0], 'b')\n",
    "plt.plot(decoded_data[0], 'r')\n",
    "plt.fill_between(np.arange(140), decoded_data[0], normal_test_data[0], color='lightcoral')\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = autoencoder.encoder(anomalous_test_data).numpy()\n",
    "decoded_data = autoencoder.decoder(encoded_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(anomalous_test_data[0], 'b')\n",
    "plt.plot(decoded_data[0], 'r')\n",
    "plt.fill_between(np.arange(140), decoded_data[0], anomalous_test_data[0], color='lightcoral')\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect anomalies\n",
    "Detect anomalies by caluclating whether the reconstruction loss is greater than a fixed threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.predict(normal_train_data)\n",
    "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_loss[None,:], bins=50)\n",
    "plt.xlabel(\"Train loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a threshold value that is one standard deviations above the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, threshold):\n",
    "    reconstructions = model(data)\n",
    "    loss = tf.keras.losses.mae(reconstructions, data)\n",
    "    return tf.math.less(loss, threshold)\n",
    "\n",
    "def print_stats(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(autoencoder, test_data, threshold)\n",
    "print_stats(preds, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
